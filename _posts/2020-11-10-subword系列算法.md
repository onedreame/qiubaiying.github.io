---
layout:     post
title:      subwordç³»åˆ—ç®—æ³•
subtitle:   ä¸åŒäºä¼ ç»Ÿåˆ†è¯çš„æ–°å¥‡æ–¹æ³•
date:       2020-11-10
author:     OD
header-img: img/post-bg-ios9-web.jpg
catalog: true
tags:
    - nlp
    - subword
    - tokenization
---

# subwordç³»åˆ—ç®—æ³•

## 1. å‰è¨€

&emsp;*nlp*é¢†åŸŸç›®å‰å·²ç»å‘å±•åˆ°ä¸€ä¸ªéå¸¸é«˜çš„å±‚æ¬¡äº†ï¼Œè¿™ä¸ªå±‚æ¬¡ä¸ä»…ä»…æ˜¯æ¨¡å‹å’Œæ•°æ®æ–¹é¢ï¼Œè¿˜æ¶Œç°å‡ºäº†å¾ˆå¤šéå¸¸å·§å¦™çš„*trick*ï¼Œè¿™ç¯‡æ–‡ç« å°±è®°å½•ä¸€ä¸‹å…³äº*tokenization*æ–¹é¢çš„å·¥ä½œã€‚

&emsp;æ‰€è°“çš„*tokenization*å…¶å®å°±æ˜¯å°†æ–‡æœ¬åˆ‡åˆ†æˆ*words*æˆ–è€…*subwords*ï¼Œç„¶åè½¬æˆ*ids*ä»¥ä¾¿æ¨¡å‹å¤„ç†ã€‚æœ€åˆçš„*nlp*åˆ†è¯éå¸¸ç®€å•ï¼Œå¯¹äºè‹±è¯­ç±»ä»¥ç©ºæ ¼åˆ†å‰²çš„è¯­è¨€æ¥è¯´ï¼Œç®€å•çš„ä»¥ç©ºæ ¼è¿›è¡Œåˆ†å‰²å°±è¡Œï¼Œä¸è¿‡è¿™æ ·ç®€å•çš„å¤„ç†è¿˜å­˜åœ¨ä¸€äº›é—®é¢˜ï¼Œæ¯”å¦‚è¯´*"Donâ€™t you love ğŸ¤— Transformers? We sure do."*,å¦‚æœç®€å•åˆ†å‰²ï¼Œé‚£ä¹ˆå°±ä¼šå¾—åˆ°*["Don't", "you", "love", "ğŸ¤—", "Transformers?", "We", "sure", "do."]*ï¼Œå¯¹äº*â€œTransformers?â€* æˆ–è€… *â€œdo.â€*è¿™æ ·çš„åˆ†è¯ç»“æœï¼Œæ˜¾ç„¶æ˜¯ä¸åˆé€‚ï¼Œæ‰€ä»¥è¿˜éœ€è¦è€ƒè™‘ä¸€ä¸‹ç¬¦å·çš„å½±å“ï¼Œè¿™å¯ä»¥é€šè¿‡ç°åœ¨å¾ˆå¤šå¼€æºå·¥å…·å®ç°ï¼Œæ¯”å¦‚*[spaCy](https://spacy.io/)*æˆ–è€…[Moses](http://www.statmt.org/moses/?n=Development.GetStarted)ç­‰ç­‰ã€‚ä¸è¿‡å¾ˆæ˜æ˜¾ä¼šæœ‰ä¸ªé—®é¢˜ï¼Œé‚£å°±æ˜¯ä½ åˆ‡åˆ†çš„è¶Šç»†ï¼Œå¾—åˆ°çš„*vocabulary*è¶Šå¤§ï¼Œå¯¹äºå­˜å‚¨ç©ºé—´çš„è¦æ±‚ä¹Ÿè¶Šå¤§ï¼Œè™½ç„¶å¤§éƒ¨åˆ†æƒ…å†µä¸‹æˆ‘ä»¬çš„è¯å…¸éƒ½ä¸ä¼šç‰¹åˆ«å¤§ï¼Œä½†æ˜¯æ€»æ˜¯å­˜åœ¨æ„å¤–æƒ…å†µï¼Œå°¤å…¶æ˜¯[Bert](https://onedreame.github.io/2020/10/31/bert%E5%AE%B6%E6%97%8F/)è¿™ç±»å¯¹äºè¯­æ–™è¶Šå¤§æ•ˆæœè¶Šå¥½çš„æ¨¡å‹çš„å‡ºç°ï¼Œæ›´æ˜¯åŠ å‰§äº†è¿™ç§æƒ…å†µã€‚å¦‚æœåˆ‡åˆ†æˆ*words*ä¼šå¯¼è‡´å†…å­˜é—®é¢˜ï¼Œé‚£ä¹ˆä¸€ä¸ªæƒ³æ³•å°±æ˜¯åš*character*çº§åˆ«çš„*tokenization*ï¼Œå¯¹äºè‹±æ–‡æ¥è¯´åªæœ‰26ä¸ªï¼Œé‚£ä¹ˆç®—ä¸Šå¤§å°å†™ä¹Ÿåªæœ‰52ä¸ªï¼Œè¿™å¯ä»¥å¤§å¤§ç¼“è§£è¿‡å¤§çš„*embedding matrix*é—®é¢˜ï¼Œä½†æ˜¯ï¼Œ*character*çº§åˆ«éœ€è¦æ¨¡å‹å»ä»åºåˆ—é¡ºåºä¸­å­¦ä¹ *word*çš„è¡¨ç¤ºï¼Œæ¨¡å‹åœ¨è¿™æ–¹é¢æ˜¾ç„¶ä¸å¦‚ç›´æ¥åˆ‡åˆ†*word*æ¥çš„ç›´è§‚ï¼Œæ‰€ä»¥è¿™ç±»æ–¹æ³•ä¼šæœ‰æ€§èƒ½æ–¹é¢çš„æŸå¤±ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ··åˆäº†*word*å’Œ*character*çš„ä¸€äº›æ–°æ–¹æ³•åº”è¿è€Œå‡ºï¼Œè¿™ç±»æ–¹æ³•è¢«ç§°ä¸º*subword*ã€‚

## 2.*subword* *tokenization*

### 2.1 [*Byte Pair Encoding*](https://arxiv.org/abs/1508.07909)

&emsp;è¿™ç§ç®—æ³•å…¶å®å¾ˆç®€å•ï¼Œä»å…¶åå­—åŸºæœ¬å¯ä»¥äº†è§£åˆ°ä¸€äº›ä¸œè¥¿ï¼Œæœ€æ˜¾çœ¼çš„å°±æ˜¯*pair*è¿™ä¸ªè¯ã€‚å®ƒçš„åšæ³•æ˜¯æ¯æ¬¡ä½¿ç”¨é¢‘æ¬¡æœ€é«˜çš„ä¸€ä¸ªå­—èŠ‚å¯¹æ¥æ›¿æ¢è¿™ä¸¤ä¸ªå­—èŠ‚ã€‚

&emsp;ç®—æ³•æµç¨‹ï¼š

1. å‡†å¤‡è¶³å¤Ÿå¤§çš„è®­ç»ƒè¯­æ–™
2. ç¡®å®šæœŸæœ›çš„*subword*è¯è¡¨å¤§å°
3. å°†å•è¯æ‹†åˆ†ä¸ºå­—ç¬¦åºåˆ—å¹¶åœ¨æœ«å°¾æ·»åŠ åç¼€â€œ </ w>â€ï¼ˆè¿™é‡Œä¸æ˜¯å¿…é¡»ï¼Œå¯ä»¥æ·»åŠ ä»»æ„å¯ä»¥ä»£è¡¨ç»“æŸçš„å­—ç¬¦ï¼‰ï¼Œç»Ÿè®¡å•è¯é¢‘ç‡ã€‚ æœ¬é˜¶æ®µçš„*subword*çš„ç²’åº¦æ˜¯å­—ç¬¦ã€‚ ä¾‹å¦‚ï¼Œâ€œ *low*â€çš„é¢‘ç‡ä¸º5ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°†å…¶æ”¹å†™ä¸ºâ€œ l o w </ w>â€ï¼š5
4. ç»Ÿè®¡æ¯ä¸€ä¸ªè¿ç»­å­—èŠ‚å¯¹çš„å‡ºç°é¢‘ç‡ï¼Œé€‰æ‹©æœ€é«˜é¢‘è€…åˆå¹¶æˆæ–°çš„*subword*
5. é‡å¤ç¬¬4æ­¥ç›´åˆ°è¾¾åˆ°ç¬¬2æ­¥è®¾å®šçš„*subword*è¯è¡¨å¤§å°æˆ–ä¸‹ä¸€ä¸ªæœ€é«˜é¢‘çš„å­—èŠ‚å¯¹å‡ºç°é¢‘ç‡ä¸º1

![](https://miro.medium.com/max/970/1*_bpIUb6YZr6DOMLAeSU2WA.png)

#### 2.1.1 æ„å»º*subword*å­—å…¸

&emsp;æœ‰è¾“å…¥çš„*word*å’Œå…¶é¢‘æ¬¡ï¼Œé¦–å…ˆå°†*word*åˆ†æˆ*character*ï¼Œå¹¶å°†*subword*è¯å…¸åˆå§‹åŒ–ä¸º*character*ï¼š

```json
{'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w e s t </w>': 6, 'w i d e s t </w>': 3}
```

&emsp;*Iter 1*, æœ€é«˜é¢‘è¿ç»­å­—èŠ‚å¯¹"*e*"å’Œ"*s*"å‡ºç°äº†6+3=9æ¬¡ï¼Œåˆå¹¶æˆ"*es*",å°†"*es*"åŠ å…¥*subword*è¯å…¸ã€‚è¾“å‡ºï¼š

```json
{'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w es t </w>': 6, 'w i d es t </w>': 3}
```

&emsp;*Iter 2*, æœ€é«˜é¢‘è¿ç»­å­—èŠ‚å¯¹"*es*"å’Œ"*t*"å‡ºç°äº†6+3=9æ¬¡, åˆå¹¶æˆ"*est*"ï¼Œå¹¶å°†"*est*"åŠ å…¥*subword*è¯å…¸ã€‚è¾“å‡ºï¼š

```json
{'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w est </w>': 6, 'w i d est </w>': 3}
```

&emsp;Iter 3, ä»¥æ­¤ç±»æ¨ï¼Œæœ€é«˜é¢‘è¿ç»­å­—èŠ‚å¯¹ä¸º"*est*"å’Œ"*</ w>*" è¾“å‡ºï¼ŒåŒæ ·åŠ å…¥*subword*è¯å…¸ï¼š

```json
{'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}
```

&emsp;â€¦â€¦

&emsp;Iter n, ç»§ç»­è¿­ä»£ç›´åˆ°è¾¾åˆ°é¢„è®¾çš„subwordè¯è¡¨å¤§å°æˆ–ä¸‹ä¸€ä¸ªæœ€é«˜é¢‘çš„å­—èŠ‚å¯¹å‡ºç°é¢‘ç‡ä¸º1ã€‚

#### 2.1.2 ç¼–ç 

&emsp;æ ¹æ®2.1.1å¾—åˆ°äº†*subword*çš„å­—å…¸ï¼Œå¯¹å•è¯å°±å¯ä»¥è¿›è¡Œç¼–ç äº†ï¼Œå°†å­—å…¸æŒ‰ç…§å­—ç¬¦é•¿åº¦é€’å‡æ’åˆ—ï¼Œç„¶åä¾æ¬¡éå†è¯å…¸ä¸­çš„*subword*ï¼Œå¦‚æœå•è¯åŒ…å«äº†è¯¥*subword*ï¼Œé‚£ä¹ˆå°±ç”¨è¯¥*subword*åŠ å…¥å•è¯çš„åˆ†è§£é›†åˆä¸­ï¼Œå¯¹å‰©ä½™çš„å­—ç¬¦è¿›è¡Œè¿­ä»£ã€‚è¿™é‡Œå°†è¯å…¸é¦–å…ˆæ’åºï¼Œä¾¿å¯ä»¥ä¿è¯æœ€å…ˆåŠ å…¥é›†åˆçš„*subword*æ˜¯æœ€åç”Ÿæˆçš„ã€‚è‹¥æœ€åè¿˜æœ‰ä¸€äº›å­—ç¬¦åºåˆ—æ— æ³•åœ¨å­—å…¸ä¸­æ‰¾åˆ°ï¼Œåˆ™æ›¿æ¢ä¸ºç‰¹æ®Šçš„*subword*ï¼Œæ¯”å¦‚*< unk >*.

&emsp;åŒæ ·ä¸¾ä¸ªä¾‹å­, å·²æœ‰æ’å¥½åºçš„è¯å…¸:

```text
[â€œerrrr</w>â€, â€œtain</w>â€, â€œmounâ€, â€œest</w>â€, â€œhighâ€, â€œthe</w>â€, â€œa</w>â€]
```

&emsp;å¯¹å•è¯"*mountain</ w>*",éå†è¯å…¸ï¼Œé¦–å…ˆçœ‹åˆ°â€œtain</ w>â€å‡ºç°åœ¨äº†å•è¯ä¸­ï¼ŒåŠ å…¥è¯¥*subword*ï¼Œå¯¹å‰©ä½™çš„"*moun*"ç»§ç»­ä¾¿åˆ©ï¼Œä¸‹ä¸€ä¸ª*subword* â€œ*moun*â€åŒ¹é…ï¼ŒåŠ å…¥ï¼Œé‚£ä¹ˆä¾¿å¾—åˆ°äº†å•è¯çš„åˆ†è§£ç»“æœä¸ºï¼š*["moun", "tain</ w>"]*ã€‚

#### 2.1.3 è§£ç 

&emsp;å°†*tokens*æ‹¼æ¥åœ¨ä¸€èµ·å³å¯ï¼Œæ³¨æ„"< /w>"ä»£è¡¨å¥å­ç»“æŸã€‚

```json
# ç¼–ç åºåˆ—
[â€œthe</w>â€, â€œhighâ€, â€œest</w>â€, â€œmounâ€, â€œtain</w>â€]

# è§£ç åºåˆ—
â€œthe</w> highest</w> mountain</w>â€
```

### 2.2 [*WordPiece*](https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf)

&emsp;*wordpiece*æ€è·¯åŸºæœ¬ä¸*Byte Pair Encoding*ï¼Œå”¯ä¸€çš„ä¸åŒå°±æ˜¯åœ¨è¿›è¡Œåˆå¹¶tokenså¯¹çš„æ—¶å€™ä¸æ˜¯ä½¿ç”¨*max frequency*ï¼Œè€Œæ˜¯ä½¿ç”¨æ¦‚ç‡æ¥ç¡®å®šåˆå¹¶å“ªä¸¤ä¸ªtokenså¯¹ï¼Œè®¡ç®—å…¬å¼ä¸ºï¼š


$$
score(A,B)=Frequency(A,b)/(Frequency(A)*Frequency(B))
$$
&emsp;å–scoreæœ€å¤§çš„tokenså¯¹è¿›è¡Œåˆå¹¶ï¼Œè¿™ä¹ˆåšä¹Ÿå¾ˆç›´è§‚ï¼Œç±»ä¼¼äºè¯è¢‹æ¨¡å‹ä¸­çš„tfidfã€‚

&emsp;*transformers* åº“ä¸­çš„*BERT tokenizer*å³æ˜¯ä½¿ç”¨çš„è¿™ç§æ–¹æ³•ï¼š

```shell
>>> from transformers import BertTokenizer
>>> tokenizer = BertTokenizer.from_pretrained("bert-base-cased")

>>> sequence = "A Titan RTX has 24GB of VRAM"
>>> tokenized_sequence = tokenizer.tokenize(sequence)
>> print(tokenized_sequence)
['A', 'Titan', 'R', '##T', '##X', 'has', '24', '##GB', 'of', 'V', '##RA', '##M']
```

&emsp;"##"ä»£è¡¨å½“å‰åºåˆ—æ˜¯å‰é¢å­—èŠ‚çš„åç¼€ï¼Œæ¯”å¦‚'R', '##T', '##X'ä¸‰ä¸ªåºåˆ—ï¼Œåº”è¯¥æ‹¼åˆä¸º"RTX".è¿™é‡Œæ²¡æœ‰ä½¿ç”¨ç»“æŸç¬¦å·ï¼Œè€Œæ˜¯ä½¿ç”¨æ‹¼æ¥ç¬¦å·æ¥ç¡®ä¿å•è¯ä½•æ—¶ç»“æŸã€‚

### 2.3 [Unigram Language Model](https://arxiv.org/pdf/1804.10959.pdf)

&emsp;è¿™æ˜¯ä¸€ç§åŸºäºè¯­è¨€æ¨¡å‹çš„*tokenization*æ–¹å¼ï¼Œä¸åŒäº*BPE*æˆ–è€…*WordPiece*,è¯¥æ–¹æ³•ä¸æ˜¯ä»åŸºç¡€å­—ç¬¦å¼€å§‹ç„¶åé€šè¿‡ä¸€å®šçš„è§„åˆ™è¿›è¡Œåˆå¹¶ï¼Œè¿™ç§æ–¹æ³•é€šè¿‡ä¸€ä¸ªå¾ˆå¤§çš„è¯å…¸å¼€å§‹ï¼Œç„¶åæ¸è¿›çš„ç¼©å‡è¯å…¸è§„æ¨¡ã€‚ç”Ÿæˆè¯å…¸çš„å…·ä½“æµç¨‹å¦‚ä¸‹ï¼š

![](https://miro.medium.com/max/712/1*9CPFFg-ilQrcPfWT8J6_ug.png)

<center>P(x):P(subwords), V:è¯å…¸, L:Likelihood to be maximized</center>

å…¶ä¸­ï¼ŒLçš„è®¡ç®—å…¬å¼ä¸ºï¼š

![](https://miro.medium.com/max/1012/1*3Uwgh_Z8uSX13jHZBoNjrA.png)

å…¶ä¸­ï¼Œ$\|D\|$ä¸ºè¯å…¸çš„å¤§å°ï¼Œ$S(x)$ä¸ºè¯*x*çš„æ‰€æœ‰å¯èƒ½çš„*tokenization*ç»„åˆé›†åˆã€‚

&emsp;å¾—åˆ°è¯å…¸ä»¥åï¼Œå°±æ˜¯è¿›è¡Œ*tokenization*äº†ï¼Œä¸è¿‡è¦æ³¨æ„çš„æ˜¯*tokenization*çš„æ–¹å¼å¯èƒ½ä¸æ­¢ä¸€ç§ï¼Œæ¯”å¦‚æœ‰ä»¥ä¸‹çš„è¯å…¸ï¼š

```python
['b', 'g', 'h', 'n', 'p', 's', 'u', 'ug', 'un', 'hug']
```

é‚£ä¹ˆå¯¹äºå•è¯â€œ*hugs*â€ ï¼Œå¯ä»¥æœ‰å¦‚ä¸‹çš„å‡ ç§ç»„åˆï¼š `['hug', 's']`, `['h', 'ug', 's']` æˆ–è€… `['h', 'u', 'g', 's']`ï¼Œç”±äºç”Ÿæˆå­—å…¸çš„æ—¶å€™æ¯ä¸ª*subword*éƒ½æ˜¯æœ‰å¯¹åº”çš„æ¦‚ç‡çš„ï¼Œæ‰€ä»¥å¯ä»¥å–tokensæ¦‚ç‡ä¹˜ç§¯æœ€å¤§çš„ç»„åˆæˆ–è€…ä»¥æ¦‚ç‡éšæœºå–ä¸€ä¸ªç»„åˆã€‚

### 2.4 [SentencePiece](https://arxiv.org/pdf/1808.06226.pdf)

&emsp;ä»”ç»†çœ‹ä¸€ä¸‹ä¸Šé¢çš„å‡ ç§æ–¹æ³•å¯ä»¥å‘ç°ï¼Œä»–ä»¬éƒ½æ˜¯éœ€è¦å®ç°æ„å»ºä¸€ä¸ªè¯å…¸ï¼Œç„¶åè¿­ä»£ï¼Œè€Œä¸”ä¸Šé¢å‡ ç§æ–¹æ³•å¯¹äºæ²¡æœ‰åˆ†éš”ç¬¦çš„è¯­è¨€æ¯”å¦‚ä¸­æ–‡æ¥è¯´å¹¶ä¸é€‚ç”¨ï¼Œå› ä¸ºå¯¹äºä¸­æ–‡è¿™ç±»è¯­è¨€æ¥è¯´ï¼Œå•ä¸ªå­—å·²ç»æ˜¯æœ€å°å•å…ƒäº†ï¼Œæ— æ³•ç»†åˆ†ä¸ºæ›´å°çš„å•å…ƒäº†ã€‚æ‰€ä»¥ä¸Šé¢å‡ ç§æ–¹æ³•éƒ½æ˜¯ä¸è¯­è¨€ç›¸å…³çš„ï¼Œé‚£ä¹ˆï¼Œæœ‰æ²¡æœ‰*language-independent* çš„æ–¹æ³•å‘¢ï¼Ÿå½“å½“å½“ï¼Œè¿™æ—¶å€™å°±åˆ°äº†*SentencePiece*ç²‰å¢¨ç™»åœºçš„æ—¶å€™äº†ã€‚

&emsp;*SentencePiece*ä¸»è¦ç”±4ä¸ªæ¨¡å—ç»„æˆï¼š*Normalizer,Trainer,Encoder,*  ä»¥åŠ*Decoder*ã€‚*Normalizer*æ˜¯ä¸€ä¸ªå°†è¯­ä¹‰ç­‰ä»·çš„Unicodeå­—ç¬¦å½’ä¸€åŒ–ä¸ºè§„èŒƒå½¢å¼çš„æ¨¡å—ã€‚  *Trainer*ä»å½’ä¸€åŒ–è¯­æ–™ä¸­è®­ç»ƒ*subword*åˆ†å‰²æ¨¡å‹ï¼Œä¸€èˆ¬ä½¿ç”¨BPEæˆ–è€…unigramã€‚ *Encoder*å†…éƒ¨æ‰§è¡Œ*Normalizer*å¯¹è¾“å…¥æ–‡æœ¬è¿›è¡Œå½’ä¸€åŒ–ï¼Œå¹¶å°†è¾“å…¥æ–‡æœ¬ç”¨*Trainer*è®­ç»ƒçš„*subword*æ¨¡å‹æ ‡è®°æˆ*subword*åºåˆ—ã€‚ *Decoder*å°†*subword*åºåˆ—è½¬æ¢ä¸ºå½’ä¸€åŒ–æ–‡æœ¬ã€‚è¿™å››ä¸ªæ¨¡å—æœ‰å¦‚ä¸‹çš„å…³ç³»ï¼š*Decoder( Encoder (Normalizer(corpus))) = Normalizer (corpus)*ï¼Œä½œè€…ç§°è¿™ä¸º**lossless tokenization**ã€‚é€šè¿‡è¿™äº›æ¨¡å—ï¼Œ*SentencePiece*å®ç°äº†ä¸€ç§ç«¯åˆ°ç«¯çš„*tokenization*ã€‚

&emsp;*transformers*åº“çš„*XlNet*ï¼Œ*ALBERT*ç­‰ä½¿ç”¨äº†è¿™ç§æ–¹æ³•ï¼š

```shell
>>> from transformers import XLNetTokenizer
>>> tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')
>>> tokenizer.tokenize("Don't you love ğŸ¤— Transformers? We sure do.")
['â–Don', "'", 't', 'â–you', 'â–love', 'â–', 'ğŸ¤—', 'â–', 'Transform', 'ers', '?', 'â–We', 'â–sure', 'â–do', '.']
```

å…¶ä¸­çš„ â€˜â–â€™ ä»£è¡¨ç©ºæ ¼ï¼Œåœ¨*decode*çš„æ—¶å€™å°†è¯¥ç¬¦å·æ›¿æ¢å›ç©ºæ ¼å³å¯ã€‚

## 3.ä¸­æ–‡å®è·µ

### 3.1 ä¸­æ–‡bpeç®—æ³•

```python
'''
ä¸­æ–‡bpeç®—æ³•
'''

def get_pairs(word):
  '''
  :param word: tokensåºåˆ—ï¼Œå¯¹äºä¸­æ–‡è€Œè¨€ï¼Œç›´æ¥tuple(text)å¤„ç†å³å¯ï¼Œå¤„ç†åä¸­æ–‡å°±å˜æˆäº†å­—çš„åºåˆ—ã€‚
  '''
  
  pairs = set()
  prev_char = word[0]
  for char in word[1:]:
      pairs.add((prev_char, char))
      prev_char = char
  return pairs


class Encoder:
    def __init__(self, encoder, bpe_merges):
      '''
      :param encoder: subwordè¯å…¸
      :param bpe_merges: éœ€è¦åˆå¹¶çš„ä¸¤ä¸ªsubwords
      '''
      
      self.encoder = encoder
      self.decoder = {v: k for k, v in self.encoder.items()}
      self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))
      self.cache = {}
      self.max_len = 0

    def bpe(self, token):
      '''
      :param token: è¦å¤„ç†çš„æ–‡æœ¬
      '''
      
      if token in self.cache:
        return self.cache[token]
      word = tuple(token)
      # è·å–æ–‡æœ¬çš„æ‰€æœ‰å­—
      
      pairs = get_pairs(word)
      if not pairs:
        return token

      while True:
        # è·å–éœ€è¦åˆå¹¶çš„subwordå¯¹ï¼Œæ³¨æ„è¿™é‡Œå·²ç»æ’åº
        
        bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float('inf')))
        # ä¸å†ç»§ç»­å¯ä»¥åˆå¹¶
        
        if bigram not in self.bpe_ranks:
          break
        first, second = bigram
        new_word = []
        # iä»£è¡¨first subwordå·¦è¾¹çš„æ‰€æœ‰subwordåºåˆ—
        
        i = 0
        while i < len(word):
          try:
            # æ‰¾åˆ°äº†å¯ä»¥åˆå¹¶çš„subword pairï¼Œå…ˆå°†first subwordå·¦è¾¹çš„æ‰€æœ‰subwordåŠ å…¥list
            
            j = word.index(first, i)
            new_word.extend(word[i:j])
            i = j
          except:
            # æ²¡æœ‰æ‰¾åˆ°å¯ä»¥åˆå¹¶çš„subword pair
            
            new_word.extend(word[i:])
            break
          
          if word[i] == first and i < len(word) - 1 and word[i + 1] == second:
            # æ‰¾åˆ°äº†å¯ä»¥åˆå¹¶çš„subwordå¯¹
            
            new_word.append(first + second)
            i += 2
          else:
            new_word.append(word[i])
            i += 1
        new_word = tuple(new_word)
        word = new_word
        if len(word) == 1:
          break
        else:
          pairs = get_pairs(word)
      word = ' '.join(word)
      self.cache[token] = word
      return word

    def encode(self, text):
        return [self.encoder.get(token, 1) for token in self.tokenize(text)]

    def decode(self, tokens):
        text = ''.join([self.decoder[token] for token in tokens])
        return text

    def tokenize(self, text):
        bpe_tokens = []
        bpe_tokens.extend(bpe_token for bpe_token in self.bpe(text).split(' '))
        return bpe_tokens

    def convert_tokens_to_ids(self, tokens):
        return [self.encoder.get(token, 1) for token in tokens]
```



## 4.æ€»ç»“

- ä¼ ç»Ÿè¯è¡¨ç¤ºæ–¹æ³•æ— æ³•å¾ˆå¥½çš„å¤„ç†æœªçŸ¥æˆ–ç½•è§çš„è¯æ±‡ï¼ˆOOVé—®é¢˜ï¼‰

- ä¼ ç»Ÿè¯*tokenization*æ–¹æ³•ä¸åˆ©äºæ¨¡å‹å­¦ä¹ è¯ç¼€ä¹‹é—´çš„å…³ç³»

- - E.g. æ¨¡å‹å­¦åˆ°çš„â€œ*old*â€, â€œ*older*â€, *and* â€œ*oldest*â€ä¹‹é—´çš„å…³ç³»æ— æ³•æ³›åŒ–åˆ°â€œ*smart*â€, â€œ*smarter*â€, and â€œ*smartest*â€ã€‚

- *Character* *embedding*ä½œä¸º*OOV*çš„è§£å†³æ–¹æ³•ç²’åº¦å¤ªç»†

- *Subword*ç²’åº¦åœ¨è¯ä¸å­—ç¬¦ä¹‹é—´ï¼Œèƒ½å¤Ÿè¾ƒå¥½çš„å¹³è¡¡*OOV*é—®é¢˜